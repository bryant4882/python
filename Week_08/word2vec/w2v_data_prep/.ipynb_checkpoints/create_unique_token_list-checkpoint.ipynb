{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a custom list of tokens\n",
    "\n",
    "This notebook was used to create a custom list of tokens-- specifically, every word in the Seinfeld script. The output is a text file containing each word exactly once, in alphabetical order, one word per line.\n",
    "\n",
    "<br>\n",
    "\n",
    "## The task\n",
    "\n",
    "Go from this (CSV containing Seinfeld dialogue and metadata):\n",
    "\n",
    "```\n",
    "...\n",
    "56,GEORGE,Im not gonna watch you do laundry.,1,S01E01,1\n",
    "57,JERRY,\"Oh, come on, be a come-with guy.\",1,S01E01,1\n",
    "58,GEORGE,\"Come on, Im tired.\",1,S01E01,1\n",
    "59,CLAIRE,\"(to Jerry) Dont worry, I gave him a little caffeine. Hell perk up.\",1,S01E01,1\n",
    "60,GEORGE,\"(panicking) Right, I knew I felt something!\",1,S01E01,1\n",
    "61,GEORGE,Jerry? I have to tell you something. This is the dullest moment Ive ever experienced.,1,S01E01,1\n",
    "62,JERRY,\"Well, look at this guy. Look, hes got everything, hes got detergents, sprays, fabric softeners.  This is not his first load.\",1,S01E01,1\n",
    "...\n",
    "```\n",
    "\n",
    "To this (a list of each word spoken in Seinfeld):\n",
    "\n",
    "```\n",
    "[...'agenda', 'agent', 'agents', 'ages', 'aggravate',...]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store all of the words in the dialogue as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "import re\n",
    "\n",
    "# Use pandas read_csv to read the csv data and\n",
    "# store it in a dataframe object\n",
    "df = read_csv('seinfeld_raw.csv')\n",
    "\n",
    "# Go through each row in the dataframe's Dialogue\n",
    "# column, and add that row's value to the string\n",
    "# called all_text\n",
    "all_text = ''\n",
    "for row in df['Dialogue'].astype(str):\n",
    "    all_text += ' ' + row\n",
    "\n",
    "# Use a regular expression to replace punctuation with\n",
    "# space throughout the text\n",
    "all_text = re.sub(r'[^a-z]', ' ', all_text.lower())\n",
    "\n",
    "# Split the string into a list of tokens, using space\n",
    "# as the separator\n",
    "all_tokens = all_text.split(' ')\n",
    "\n",
    "# Remove empty tokens from list\n",
    "all_tokens = list(filter(None, all_tokens))\n",
    "\n",
    "# Sort the list\n",
    "all_tokens.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a de-deuplicated version of the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new list that holds exactly one of each token\n",
    "unique_tokens = []\n",
    "for token in all_tokens:\n",
    "    if token not in unique_tokens:\n",
    "        unique_tokens.append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the deduplicated items to a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write each item as as a newline in the output text file\n",
    "with open('seinfeld_tokens.txt', 'w') as f:\n",
    "    for token in unique_tokens:\n",
    "        f.write(token + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## More sophisticated processing...\n",
    "\n",
    "Not used in today's class example, but good to know about..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "print(tokenizer.tokenize('Hello.'))\n",
    "print(tokenizer.tokenize('www.newschool.edu/'))\n",
    "print(tokenizer.tokenize('So...what\\'s up?!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('dog'))\n",
    "print(lemmatizer.lemmatize('dogs'))\n",
    "\n",
    "print(lemmatizer.lemmatize('goose'))\n",
    "print(lemmatizer.lemmatize('geese'))\n",
    "\n",
    "print(lemmatizer.lemmatize('went', 'v'))\n",
    "print(lemmatizer.lemmatize('going', 'v'))\n",
    "\n",
    "print(lemmatizer.lemmatize('smaller', 'a'))\n",
    "print(lemmatizer.lemmatize('smallest', 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "print(stemmer.stem('going'))\n",
    "print(stemmer.stem('bicycling'))\n",
    "\n",
    "print(stemmer.stem('bananas'))\n",
    "print(stemmer.stem('apples'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
