{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>What is word2vec?</h1>\n",
    "<p>Word2vec is a machine learning method to efficiently create `word embeddings`.</p>\n",
    "\n",
    "Every word is transformed into a numeral sequence or an array of numbers: \n",
    "`multi-dimensional meaning representations of a word`\n",
    "\n",
    "The vector data of one word `banana` can look like:\n",
    "```\n",
    "    array([2.02280000e-01,  -7.66180009e-02,   3.70319992e-01,\n",
    "       3.28450017e-02,  -4.19569999e-01,   7.20689967e-02,\n",
    "      -3.74760002e-01,   5.74599989e-02,  -1.24009997e-02,\n",
    "       5.29489994e-01,  -5.23800015e-01,  -1.97710007e-01,\n",
    "      -3.41470003e-01,   5.33169985e-01,  -2.53309999e-02,\n",
    "       1.73800007e-01,   1.67720005e-01,   8.39839995e-01,\n",
    "       5.51070012e-02,   1.05470002e-01,   3.78719985e-01,\n",
    "       2.42750004e-01,   1.47449998e-02,   5.59509993e-01,\n",
    "       1.25210002e-01,  -6.75960004e-01,   3.58420014e-01,\n",
    "       # ... and so on ...\n",
    "       3.66849989e-01,   2.52470002e-03,  -6.40089989e-01,\n",
    "      -2.97650009e-01,   7.89430022e-01,   3.31680000e-01,\n",
    "      -1.19659996e+00,  -4.71559986e-02,   5.31750023e-01], dtype=float32)\n",
    "    \n",
    "```\n",
    "To better visualize what w2v: [illustrated word2vec](http://jalammar.github.io/illustrated-word2vec/)\n",
    "\n",
    "By knowing vector data of words, we can do many interesting things:\n",
    "1. Similarity among words\n",
    "1. Linear algebra among words \n",
    "1. Recommendation engines\n",
    "1. ...\n",
    "\n",
    "<h1> What are we doing today?</h1>\n",
    "\n",
    "1. Use the Seinfeld wordlist we just prepared\n",
    "1. Use spaCy word2vec model to vectorize a single word then many words\n",
    "1. Explore semantic similarity among words using math\n",
    "1. Create a fun method that transform any song title or lyrics into Seinfeld-themed word combination\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take a look at our words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18209 unique word tokens in our Seinfeld word collection.\n",
      "['plotting', 'ploughing', 'ploy', 'pluck', 'plug', 'plugged', 'pluggin', 'plugola', 'plugs', 'plum']\n"
     ]
    }
   ],
   "source": [
    "# We will import the prepared word list first then split them into a list\n",
    "file = open('../data_prep/seinfeld_tokens.txt','r')\n",
    "seinfeld_tokens = file.read().lower().split('\\n')\n",
    "print(\"There are %d unique word tokens in our Seinfeld word collection.\" %(len(seinfeld_tokens)))\n",
    "\n",
    "# Let's randomly print our 10 of them\n",
    "print(seinfeld_tokens[12000:12010])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize words with spaCY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use spaCy's w2v model - a Natural Language Processing library that contains pre-trained vector data of common vocabularies.\n",
    "\n",
    "> spaCy w2v documentation [here](https://spacy.io/usage/vectors-similarity)\n",
    "\n",
    "Before we start, if you haven't downloaded spaCy, remember to pip install. Also download the medium size spaCy model, which contains word vectors data.\n",
    "\n",
    "```python\n",
    "!pip install spacy\n",
    "python -m spacy download en_core_web_md\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy and load spaCy medium model\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look up vector data of words\n",
    "nlp.vocab[\"banana\"].vector\n",
    "# Define a function for getting word vector data\n",
    "def vec(word):\n",
    "    return nlp.vocab[word].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fleas\n"
     ]
    }
   ],
   "source": [
    "# Look up vector of random words from our seinfeld_tokens \n",
    "from random import choice\n",
    "random_seinfeld_word = choice(seinfeld_tokens)\n",
    "print(random_seinfeld_word)\n",
    "#vec(random_seinfeld_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Math\n",
    "\n",
    "If words are represented in vectors, then we can use vector math on them?\n",
    "\n",
    "Imagine:\n",
    "1. \"King\" - \"Man\"\n",
    "1. \"Apple\" + \"Purple\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Similarity\n",
    "\n",
    "Since we have vector data of words, it also means we have positions of these words in a multi-dimensional space(100-300 dimensions)\n",
    "All these words are located in relation to each other in a larger word context. In a nutshall, when the machine is training to assign/identify positional data of each word, it's looking at where and what other words this word is usually with. \n",
    "\n",
    "If all these words are two dimensional, with only an x and y coordinate, we can probably imagine them scattered on a 2-D coordinate. \n",
    "Anna and I made a little game a while ago where we used the same Seinfeld raw text and plotted everything on a two-dimensional vector space. You can find that multi-dimensional reduction process [here](https://github.com/lanzhang76/toast/blob/master/word-processing/w2v_SNE_graph/w2v_t-SNE_Seinfeld.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare similarity among words by comparing their distances with each other.\n",
    "\n",
    "Popular methods for calculating vector distance are:\n",
    "1. Euclidean Distance (good for two-dimensional vector)\n",
    "1. Cosine Similary (good for multi-dimensional vector)\n",
    "\n",
    "\n",
    "<img src=\"../w2v/img.jpeg\" alt=\"drawing\" width=\"400\" style=\"float: left;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning Cosine Similarity equation into a function\n",
    "# We will need to download and import numpy in order to use following:\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Define a function that outputs cosine similarity of any two given vectors\n",
    "def cosine(v1, v2):\n",
    "    if norm(v1) > 0 and norm(v2) > 0:\n",
    "        return dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.50557566\n",
      "0.15468664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Higher number means more similar\n",
    "print(cosine(vec(\"milk\"),vec(\"water\")))\n",
    "print(cosine(vec(\"milk\"),vec(\"car\")))\n",
    "cosine(vec(\"milk\"),vec(\"water\")) > cosine(vec(\"milk\"),vec(\"car\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that iterates through token_list\n",
    "def spacy_closest(token_list, # Given token list \n",
    "                  target_word_vec, # Any word vector\n",
    "                  n=10 # by default 10 closest words\n",
    "                 ):\n",
    "    \n",
    "    # compare every word to the target_word, outputs a sorted list of n cloesets words.\n",
    "    return sorted(token_list,\n",
    "                  key=lambda x: cosine(target_word_vec, vec(x)), # lamda function is a shortened function \n",
    "                  reverse=True # True is ascending \n",
    "                 )[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat',\n",
       " 'cats',\n",
       " 'dog',\n",
       " 'kitty',\n",
       " 'pet',\n",
       " 'poodle',\n",
       " 'puppy',\n",
       " 'retriever',\n",
       " 'dogs',\n",
       " 'rabbit']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get 10 closest words appeared in Seinfeld to the target word \"cat\"\n",
    "spacy_closest(seinfeld_tokens,vec(\"cat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bodega',\n",
       " 'deli',\n",
       " 'delicatessen',\n",
       " 'delicatessens',\n",
       " 'knish',\n",
       " 'appetizers',\n",
       " 'pastrami',\n",
       " 'pastries',\n",
       " 'sandwiches',\n",
       " 'artisan',\n",
       " 'bakeries',\n",
       " 'bakery',\n",
       " 'supermarket',\n",
       " 'grocery',\n",
       " 'pizza']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_closest(seinfeld_tokens,vec(\"deli\"),15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seinfeld-themed song title transformer\n",
    "\n",
    "Let's create a fun method that transform any song title or lyrics into Seinfeld-themed word combination using our cloesest neighbors method we just created\n",
    "\n",
    "Make sure you have following libraries imported&downloaded:\n",
    "\n",
    "```python\n",
    "    from numpy import dot\n",
    "    from numpy.linalg import norm\n",
    "    from random import choice\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that takes in a sentence and a list of tokens\n",
    "def seinfeldTransformer(song_title,word_tokens,num = 2):\n",
    "    \n",
    "    # Vectorize Function\n",
    "    \n",
    "    # Cosine Similarity Function\n",
    "    \n",
    "    # Closest Neighbor Function\n",
    "    \n",
    "    # Replace current sentence input with a random most-similar words\n",
    "    def getNewTitle(sen, li):\n",
    "        word_list = sen.split(\" \")\n",
    "        new_list = []\n",
    "        for word in word_list:\n",
    "            replace_word = choice(spacy_closest(li,vec(word),10))\n",
    "            new_list.append(replace_word)\n",
    "        return ' '.join(new_list)\n",
    "    \n",
    "    # Generate\n",
    "    print('\"%s\" can also be called:' % (song_title))\n",
    "    for i in range(num):\n",
    "        print(\"---\")\n",
    "        print(\"%d. %s\" % (i+1,getNewTitle(song_title,word_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Another one bites the dust\" can also be called:\n",
      "---\n",
      "1. an it antidote its pebble\n",
      "---\n",
      "2. another only treats one debris\n",
      "---\n",
      "3. least that mauled that grime\n",
      "---\n",
      "4. a even fleas entire sand\n",
      "---\n",
      "5. one it bite its debris\n"
     ]
    }
   ],
   "source": [
    "# Queen?\n",
    "seinfeldTransformer(\"Another one bites the dust\",seinfeld_tokens,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"This guy is in love with you\" can also be called:\n",
      "---\n",
      "1. one jocks being within know well sure\n",
      "---\n",
      "2. it dude comes the loving together sure\n",
      "---\n",
      "3. one dude it in loooove up can\n",
      "---\n",
      "4. one guy only around really both get\n",
      "---\n",
      "5. that someone comes the loves with you\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "seinfeldTransformer(\"This guy is in love with you\",seinfeld_tokens,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"We are the champions\" can also be called:\n",
      "---\n",
      "1. they are part victory\n",
      "---\n",
      "2. let these one decathlon\n",
      "---\n",
      "3. what these part finals\n",
      "---\n",
      "4. they are part triumph\n",
      "---\n",
      "5. we those into decathlon\n"
     ]
    }
   ],
   "source": [
    "# more Queen?\n",
    "seinfeldTransformer(\"We are the champions\",seinfeld_tokens,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A fox is chasing a chicken\" can also be called:\n",
      "---\n",
      "1. second rabbit is pouncing another marsala\n",
      "---\n",
      "2. one wolf one tailing is brisket\n",
      "---\n",
      "3. another skunk is chasing the pork\n",
      "---\n",
      "4. kind skunk be pouncing something chowder\n",
      "---\n",
      "5. another rabbit comes chasing first meat\n"
     ]
    }
   ],
   "source": [
    "# or any sentence really:\n",
    "seinfeldTransformer(\"A fox is chasing a chicken\",seinfeld_tokens,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
