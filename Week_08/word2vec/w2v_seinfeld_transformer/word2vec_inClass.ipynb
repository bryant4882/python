{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>What is word2vec?</h1>\n",
    "<p>Word2vec is a machine learning method to efficiently create `word embeddings`.</p>\n",
    "\n",
    "Every word is transformed into a numeral sequence or an array of numbers: \n",
    "`multi-dimensional meaning representations of a word`\n",
    "\n",
    "The vector data of one word `banana` can look like:\n",
    "```\n",
    "    array([2.02280000e-01,  -7.66180009e-02,   3.70319992e-01,\n",
    "       3.28450017e-02,  -4.19569999e-01,   7.20689967e-02,\n",
    "      -3.74760002e-01,   5.74599989e-02,  -1.24009997e-02,\n",
    "       5.29489994e-01,  -5.23800015e-01,  -1.97710007e-01,\n",
    "      -3.41470003e-01,   5.33169985e-01,  -2.53309999e-02,\n",
    "       1.73800007e-01,   1.67720005e-01,   8.39839995e-01,\n",
    "       5.51070012e-02,   1.05470002e-01,   3.78719985e-01,\n",
    "       2.42750004e-01,   1.47449998e-02,   5.59509993e-01,\n",
    "       1.25210002e-01,  -6.75960004e-01,   3.58420014e-01,\n",
    "       # ... and so on ...\n",
    "       3.66849989e-01,   2.52470002e-03,  -6.40089989e-01,\n",
    "      -2.97650009e-01,   7.89430022e-01,   3.31680000e-01,\n",
    "      -1.19659996e+00,  -4.71559986e-02,   5.31750023e-01], dtype=float32)\n",
    "    \n",
    "```\n",
    "To better visualize what w2v: [illustrated word2vec](http://jalammar.github.io/illustrated-word2vec/)\n",
    "\n",
    "By knowing vector data of words, we can do many interesting things:\n",
    "1. Similarity among words\n",
    "1. Linear algebra among words \n",
    "1. Recommendation engines\n",
    "1. ...\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h1> What are we doing today?</h1>\n",
    "\n",
    "1. Use preprepared word lists (How we prepared [Seinfeld word list](http://localhost:8891/notebooks/data_prep/create_unique_token_list.ipynb))\n",
    "1. Use spaCy word2vec model to vectorize a single word then many words\n",
    "1. Explore semantic similarity among words using vector math\n",
    "1. Create a fun method that transform any song title or lyrics into Seinfeld-themed titles\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n",
    "\n",
    "# Take a look at our words\n",
    "\n",
    "We will import the prepared word list first then split them into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emma\n",
    "# file_emma = open('../w2v_data_prep/jane_austen_emma.txt','r')\n",
    "# emma_tokens = file_emma.read().lower().split('\\n')\n",
    "# print(\"There are %d unique word tokens in our Emma word collection.\" %(len(emma_tokens)))\n",
    "# print(emma_tokens[:10])\n",
    "\n",
    "# Seinfeld\n",
    "file = open('../w2v_data_prep/seinfeld_tokens.txt','r')\n",
    "seinfeld_tokens = file.read().lower().split('\\n')\n",
    "print(\"There are %d unique word tokens in our Seinfeld word collection.\" %(len(seinfeld_tokens)))\n",
    "\n",
    "# Let's randomly print our 10 of them\n",
    "print(seinfeld_tokens[12000:12010])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n",
    "\n",
    "# Vectorize words with spaCy\n",
    "\n",
    "We will use spaCy's w2v model - a Natural Language Processing library that contains pre-trained vector data of common vocabularies.\n",
    "\n",
    "> spaCy w2v documentation [here](https://spacy.io/usage/vectors-similarity)\n",
    "\n",
    "Before we start, if you haven't downloaded spaCy, remember to pip install. Also download the medium size spaCy model, which contains word vectors data.\n",
    "\n",
    "```python\n",
    "!pip install spacy\n",
    "python -m spacy download en_core_web_md\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy and load spaCy medium model\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for getting word vector data\n",
    "def vec(word):\n",
    "    return nlp.vocab[word].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look up vector of random words from our seinfeld_tokens \n",
    "from random import choice\n",
    "random_seinfeld_word = choice(seinfeld_tokens)\n",
    "print(random_seinfeld_word)\n",
    "vec(random_seinfeld_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "<hr>\n",
    "\n",
    "# Vector Math\n",
    "\n",
    "If words are represented in vectors, then we can use vector math on them?\n",
    "\n",
    "Imagine:\n",
    "1. \"King\" - \"Man\"\n",
    "1. \"Apple\" + \"Purple\"\n",
    "\n",
    "# Word Similarity\n",
    "\n",
    "Since we have vector data of words, it also means we have positions of these words in a multi-dimensional space(100-300 dimensions)\n",
    "All these words are located in relation to each other in a larger word context. In a nutshall, when the machine is training to assign/identify positional data of each word, it's looking at where and what other words this word is usually with. \n",
    "\n",
    "If all these words are two dimensional, with only an x and y coordinate, we can probably imagine them scattered on a 2-D coordinate. \n",
    "Anna and I made a little game a while ago where we used the same Seinfeld raw text and plotted everything on a two-dimensional vector space. You can find that multi-dimensional reduction process [here](https://github.com/parsons-python-summer-2020/python/tree/master/Week_08/word2vec/w2v_2d_plotting).\n",
    "\n",
    "We can also compare similarity among words by comparing their distances with each other.\n",
    "\n",
    "Popular methods for calculating vector distance are:\n",
    "1. Euclidean Distance (good for two-dimensional vector)\n",
    "1. Cosine Similary (good for multi-dimensional vector)\n",
    "\n",
    "\n",
    "<img src=\"./img.jpeg\" alt=\"drawing\" width=\"400\" style=\"float: left;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning Cosine Similarity equation into a function\n",
    "# We will need to download and import numpy in order to use following:\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Define a function that outputs cosine similarity of any two given vectors\n",
    "def cosine(v1, v2):\n",
    "    if norm(v1) > 0 and norm(v2) > 0:\n",
    "        return dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher number means more similar\n",
    "\n",
    "print(cosine(vec(\"milk\"),vec(\"water\")))\n",
    "print(cosine(vec(\"milk\"),vec(\"car\")))\n",
    "cosine(vec(\"milk\"),vec(\"water\")) > cosine(vec(\"milk\"),vec(\"car\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that iterates through token_list\n",
    "\n",
    "def spacy_closest(token_list, # Given token list \n",
    "                  target_word_vec, # Any word vector\n",
    "                  n=10 # by default 10 closest words\n",
    "                 ):\n",
    "    \n",
    "    # compare every word to the target_word, outputs a sorted list of n cloesets words.\n",
    "    return sorted(token_list,\n",
    "                  key=lambda x: cosine(target_word_vec, vec(x)), \n",
    "                  reverse=True # True is ascending \n",
    "                 )[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda function is a shortened function \n",
    "# lambda arguments : expression\n",
    "plusTen = lambda a : a + 10\n",
    "print(plusTen(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 10 closest words to the target word \"cat\" from Seinfeld word list\n",
    "spacy_closest(seinfeld_tokens,vec(\"cat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_closest(seinfeld_tokens,vec(\"deli\"),15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seinfeld-themed song title transformer\n",
    "\n",
    "Let's create a fun method that transform any song title or lyrics into Seinfeld-themed word combination using our cloesest neighbors method we just created\n",
    "\n",
    "Make sure you have following libraries imported&downloaded:\n",
    "\n",
    "```python\n",
    "    from numpy import dot\n",
    "    from numpy.linalg import norm\n",
    "    from random import choice\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that takes in a sentence and a list of tokens\n",
    "def seinfeldTransformer(song_title,word_tokens,num = 2):\n",
    "    \n",
    "    # Vectorize Function\n",
    " \n",
    "    \n",
    "    # Cosine Similarity Function\n",
    "\n",
    "    \n",
    "    # Closest Neighbor Function\n",
    "   \n",
    "    \n",
    "    # Replace current sentence input with a random most-similar word\n",
    "    \n",
    "    def getNewTitle(sen, li):\n",
    "        word_list = sen.split(\" \")\n",
    "        new_list = []\n",
    "        for word in word_list:\n",
    "            replace_word = choice(spacy_closest(li,vec(word),10))\n",
    "            new_list.append(replace_word)\n",
    "        return ' '.join(new_list)\n",
    "    \n",
    "    \n",
    "    # Generate!\n",
    "    print('\"%s\" can also be called:' % (song_title))\n",
    "    for i in range(num):\n",
    "        print(\"---\")\n",
    "        print(\"%d. %s\" % (i+1, getNewTitle(song_title,word_tokens)))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seinfeldTransformer(\"Another one bites the dust\",seinfeld_tokens,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or any sentence really:\n",
    "seinfeldTransformer(\"A fox is chasing a chicken\",seinfeld_tokens,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more Queen?\n",
    "seinfeldTransformer(\"We Are the Champions\",seinfeld_tokens,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need a new branding for your band?\n",
    "seinfeldTransformer(\"Red Hot Chilli Peppers\",seinfeld_tokens,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n",
    "\n",
    "# Going Further\n",
    "\n",
    "- Use a different word list we provided in the folder: `../w2v_data_prep/jane_austen_emma.txt`\n",
    "    - There are notebooks in the same folder that demonstrate how the words are prepared.\n",
    "- Explore other Natural Language Processing Dataset through this [git repo](https://github.com/niderhoff/nlp-datasets)\n",
    "- Read Allison Parrish [\"Understanding Word Vectors\"](https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469)\n",
    "- If you want to read more about word vector, check out [Illustrated word2vec](http://jalammar.github.io/illustrated-word2vec/)\n",
    "- If you are interested in visualizing word vectors on a 2D vector space, check out [this notebook](https://github.com/parsons-python-summer-2020/python/tree/master/Week_08/word2vec/w2v_2d_plotting) by Anna & Lan\n",
    "\n",
    "Preview of a reduced 2D word vector space:\n",
    "<img src=\"./t-SNE.png\" alt=\"drawing\" width=\"800\" style=\"float: left;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
