{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>What is word2vec?</h1>\n",
    "<p>Word2vec is a machine learning method to efficiently create `word embeddings`.</p>\n",
    "\n",
    "Every word is transformed into a numeral sequence or an array of numbers: \n",
    "`multi-dimensional meaning representations of a word`\n",
    "\n",
    "The vector data of one word `banana` can look like:\n",
    "```\n",
    "    array([2.02280000e-01,  -7.66180009e-02,   3.70319992e-01,\n",
    "       3.28450017e-02,  -4.19569999e-01,   7.20689967e-02,\n",
    "      -3.74760002e-01,   5.74599989e-02,  -1.24009997e-02,\n",
    "       5.29489994e-01,  -5.23800015e-01,  -1.97710007e-01,\n",
    "      -3.41470003e-01,   5.33169985e-01,  -2.53309999e-02,\n",
    "       1.73800007e-01,   1.67720005e-01,   8.39839995e-01,\n",
    "       5.51070012e-02,   1.05470002e-01,   3.78719985e-01,\n",
    "       2.42750004e-01,   1.47449998e-02,   5.59509993e-01,\n",
    "       1.25210002e-01,  -6.75960004e-01,   3.58420014e-01,\n",
    "       # ... and so on ...\n",
    "       3.66849989e-01,   2.52470002e-03,  -6.40089989e-01,\n",
    "      -2.97650009e-01,   7.89430022e-01,   3.31680000e-01,\n",
    "      -1.19659996e+00,  -4.71559986e-02,   5.31750023e-01], dtype=float32)\n",
    "    \n",
    "```\n",
    "To better visualize what w2v: [illustrated word2vec](http://jalammar.github.io/illustrated-word2vec/)\n",
    "\n",
    "By knowing vector data of words, we can do many interesting things:\n",
    "1. Similarity among words\n",
    "1. Linear algebra among words \n",
    "1. Recommendation engines\n",
    "1. ...\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h1> What are we doing today?</h1>\n",
    "\n",
    "1. Use preprepared word lists (How we prepared [Seinfeld word list](http://localhost:8891/notebooks/data_prep/create_unique_token_list.ipynb))\n",
    "1. Use spaCy word2vec model to vectorize a single word then many words\n",
    "1. Explore semantic similarity among words using vector math\n",
    "1. Create a fun method that transform any song title or lyrics into Seinfeld-themed titles\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr>\n",
    "\n",
    "# Take a look at our words\n",
    "We will import the prepared word list first then split them into a list\n",
    "I've also prepared a different text file for you to play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18209 unique word tokens in our Seinfeld word collection.\n",
      "['plotting', 'ploughing', 'ploy', 'pluck', 'plug', 'plugged', 'pluggin', 'plugola', 'plugs', 'plum']\n"
     ]
    }
   ],
   "source": [
    "# Emma\n",
    "# file_emma = open('../w2v_data_prep/jane_austen_emma.txt','r')\n",
    "# emma_tokens = file_emma.read().lower().split('\\n')\n",
    "# print(\"There are %d unique word tokens in our Emma word collection.\" %(len(emma_tokens)))\n",
    "# print(emma_tokens[:10])\n",
    "\n",
    "\n",
    "\n",
    "#Seinfeld\n",
    "file = open('../w2v_data_prep/seinfeld_tokens.txt','r')\n",
    "seinfeld_tokens = file.read().lower().split('\\n')\n",
    "print(\"There are %d unique word tokens in our Seinfeld word collection.\" %(len(seinfeld_tokens)))\n",
    "\n",
    "# Let's randomly print our 10 of them\n",
    "print(seinfeld_tokens[12000:12010])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize words with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use spaCy's w2v model - a Natural Language Processing library that contains pre-trained vector data of common vocabularies.\n",
    "\n",
    "> spaCy w2v documentation [here](https://spacy.io/usage/vectors-similarity)\n",
    "\n",
    "Before we start, if you haven't downloaded spaCy, remember to pip install. Also download the medium size spaCy model, which contains word vectors data.\n",
    "\n",
    "```python\n",
    "!pip install spacy\n",
    "python -m spacy download en_core_web_md\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy and load spaCy medium model\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look up vector data of words\n",
    "nlp.vocab[\"banana\"].vector\n",
    "# Define a function for getting word vector data\n",
    "def vec(word):\n",
    "    return nlp.vocab[word].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roses\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.2692   ,  0.45273  , -0.69142  , -1.168    ,  0.14458  ,\n",
       "       -0.89465  , -0.28544  , -0.07494  , -0.49658  ,  1.0735   ,\n",
       "       -0.039109 ,  0.1325   ,  0.0424   , -0.35718  , -0.028951 ,\n",
       "       -0.39836  , -0.43131  ,  1.0509   ,  0.52473  , -0.31442  ,\n",
       "       -0.24403  ,  0.38476  , -0.6288   , -0.17396  , -0.26656  ,\n",
       "       -0.024481 , -0.24497  ,  0.33608  ,  0.067374 ,  0.014087 ,\n",
       "        0.010409 ,  0.18535  , -0.063205 , -0.3312   , -0.24169  ,\n",
       "        0.43697  ,  0.10615  , -0.21458  , -0.075054 , -0.20403  ,\n",
       "        0.27611  ,  0.069161 ,  0.5394   , -0.04365  ,  0.72591  ,\n",
       "        0.043186 , -0.20793  , -0.78139  ,  0.61867  ,  0.222    ,\n",
       "       -0.47597  , -0.41733  , -0.2913   , -0.37086  ,  0.48167  ,\n",
       "       -0.046105 , -0.053051 ,  0.2931   , -0.62461  ,  0.16999  ,\n",
       "       -0.053034 , -0.04266  , -0.38913  , -0.22146  , -0.0040796,\n",
       "        0.35693  , -0.31678  , -0.2486   , -0.1179   , -0.25526  ,\n",
       "       -0.72506  ,  0.40103  , -0.12096  ,  0.37142  ,  0.27284  ,\n",
       "        0.25997  ,  0.29745  ,  0.036745 , -0.13446  , -0.41974  ,\n",
       "       -0.44345  ,  0.41444  ,  0.22921  , -0.23403  , -0.42904  ,\n",
       "       -0.8422   ,  1.003    ,  0.83543  ,  0.14158  , -0.41525  ,\n",
       "        0.35141  ,  0.0066326,  0.93834  ,  0.28119  ,  0.12279  ,\n",
       "        0.89389  , -0.1998   ,  0.0032227, -0.14547  , -0.28302  ,\n",
       "       -0.093091 ,  0.057052 ,  0.31444  , -0.37441  , -0.33685  ,\n",
       "       -0.35089  , -0.063661 ,  0.17202  ,  0.62571  ,  0.24149  ,\n",
       "       -0.13274  ,  0.21052  ,  0.39566  ,  0.32398  , -0.12888  ,\n",
       "       -0.19278  , -0.47249  ,  0.31614  ,  0.066234 , -0.25625  ,\n",
       "        0.054112 ,  0.1172   , -0.10888  ,  0.49862  , -0.33371  ,\n",
       "       -0.22807  ,  0.13331  ,  0.42141  ,  0.30505  ,  0.19248  ,\n",
       "        0.023994 ,  0.0095459, -0.0060352,  0.22479  ,  0.25237  ,\n",
       "       -0.033001 ,  0.26492  ,  0.54565  ,  0.27859  ,  0.31557  ,\n",
       "       -2.2943   ,  0.35024  , -0.15037  ,  0.051925 , -0.89702  ,\n",
       "        0.33965  , -0.19402  ,  0.24937  ,  0.47768  ,  0.57587  ,\n",
       "        0.50112  ,  0.044343 ,  0.56779  ,  0.44292  , -0.56036  ,\n",
       "       -0.37862  , -0.041327 , -0.22382  , -0.40488  ,  0.52247  ,\n",
       "       -0.11399  ,  0.50999  , -0.072117 ,  0.07311  ,  0.32884  ,\n",
       "        0.21719  ,  0.16809  , -0.83587  , -0.0238   ,  0.4267   ,\n",
       "       -0.56572  ,  0.043347 ,  0.21047  , -0.072788 ,  0.45045  ,\n",
       "        0.36453  , -0.41269  , -0.026226 , -0.056664 , -0.15193  ,\n",
       "        0.099522 ,  0.20693  ,  0.50478  , -0.10473  , -0.18039  ,\n",
       "       -0.66931  , -0.37242  , -0.61593  ,  0.14496  , -0.16791  ,\n",
       "        0.27242  ,  0.075849 , -0.36543  , -0.34866  ,  0.094542 ,\n",
       "        0.18662  , -0.1351   , -0.2323   , -0.23957  ,  0.34837  ,\n",
       "        0.014912 , -0.19123  ,  0.024037 ,  0.089079 ,  0.11913  ,\n",
       "       -0.11794  ,  0.91319  , -0.094987 ,  0.090984 , -0.5196   ,\n",
       "       -0.059501 ,  0.93736  ,  0.32252  , -0.33751  , -0.12888  ,\n",
       "        0.32562  , -0.35494  , -0.31308  ,  0.14727  , -0.58475  ,\n",
       "       -0.41048  , -0.2491   ,  0.45635  , -0.1048   , -0.011091 ,\n",
       "        0.22747  ,  0.30366  , -0.17465  , -0.55618  ,  0.48405  ,\n",
       "       -0.09112  ,  0.14435  , -0.045332 , -0.084252 , -0.44713  ,\n",
       "       -0.49998  ,  0.057041 , -0.70285  , -0.077348 ,  0.11633  ,\n",
       "       -0.85617  ,  0.14227  , -0.42706  , -0.41604  , -0.14676  ,\n",
       "        0.24651  , -0.31638  ,  0.64033  ,  0.13096  ,  0.11524  ,\n",
       "       -0.21605  ,  0.414    , -0.37123  ,  0.17289  ,  0.22352  ,\n",
       "        0.30239  , -0.10679  , -0.19971  , -0.39965  ,  0.24426  ,\n",
       "       -0.057211 ,  0.010475 , -0.15424  , -0.26355  , -0.21832  ,\n",
       "       -0.16028  ,  0.79057  ,  0.085057 , -0.71362  ,  0.45912  ,\n",
       "       -0.87667  ,  0.0056823,  0.12587  ,  0.16227  , -0.23496  ,\n",
       "        0.22894  , -0.42332  ,  0.10826  ,  0.16849  ,  0.16232  ,\n",
       "        0.44496  , -0.55545  , -0.084256 , -0.85562  ,  0.73981  ,\n",
       "        0.16962  ,  0.15248  ,  0.047817 , -0.37341  , -0.75946  ,\n",
       "       -0.088831 ,  0.057645 ,  0.29575  , -0.10714  , -0.33346  ,\n",
       "       -0.17066  ,  0.11084  , -0.40797  ,  0.30105  ,  0.48933  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look up vector of random words from our seinfeld_tokens \n",
    "from random import choice\n",
    "random_seinfeld_word = choice(seinfeld_tokens)\n",
    "print(random_seinfeld_word)\n",
    "vec(random_seinfeld_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Math\n",
    "\n",
    "If words are represented in vectors, then we can use vector math on them?\n",
    "\n",
    "Imagine:\n",
    "1. \"King\" - \"Man\"\n",
    "1. \"Apple\" + \"Purple\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Similarity\n",
    "\n",
    "Since we have vector data of words, it also means we have positions of these words in a multi-dimensional space(100-300 dimensions)\n",
    "All these words are located in relation to each other in a larger word context. In a nutshall, when the machine is training to assign/identify positional data of each word, it's looking at where and what other words this word is usually with. \n",
    "\n",
    "If all these words are two dimensional, with only an x and y coordinate, we can probably imagine them scattered on a 2-D coordinate. \n",
    "Anna and I made a little game a while ago where we used the same Seinfeld raw text and plotted everything on a two-dimensional vector space. You can find that multi-dimensional reduction process [here](https://github.com/parsons-python-summer-2020/python/tree/master/Week_08/word2vec/w2v_2d_plotting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare similarity among words by comparing their distances with each other.\n",
    "\n",
    "Popular methods for calculating vector distance are:\n",
    "1. Euclidean Distance (good for two-dimensional vector)\n",
    "1. Cosine Similary (good for multi-dimensional vector)\n",
    "\n",
    "\n",
    "<img src=\"./img.jpeg\" alt=\"drawing\" width=\"400\" style=\"float: left;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning Cosine Similarity equation into a function\n",
    "# We will need to download and import numpy in order to use following:\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Define a function that outputs cosine similarity of any two given vectors\n",
    "def cosine(v1, v2):\n",
    "    if norm(v1) > 0 and norm(v2) > 0:\n",
    "        return dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.50557566\n",
      "0.15468664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Higher number means more similar\n",
    "print(cosine(vec(\"milk\"),vec(\"water\")))\n",
    "print(cosine(vec(\"milk\"),vec(\"car\")))\n",
    "cosine(vec(\"milk\"),vec(\"water\")) > cosine(vec(\"milk\"),vec(\"car\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that iterates through token_list\n",
    "\n",
    "def spacy_closest(token_list, # Given token list \n",
    "                  target_word_vec, # Any word vector\n",
    "                  n=10 # by default 10 closest words\n",
    "                 ):\n",
    "    \n",
    "    # compare every word to the target_word, outputs a sorted list of n cloesets words.\n",
    "    return sorted(token_list,\n",
    "                  key=lambda x: cosine(target_word_vec, vec(x)), # lamda function is a shortened function \n",
    "                  reverse=True # True is ascending \n",
    "                 )[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "# lambda function is a shortened function \n",
    "# lambda arguments : expression\n",
    "plusTen = lambda a : a + 10\n",
    "print(plusTen(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat',\n",
       " 'cats',\n",
       " 'dog',\n",
       " 'kitty',\n",
       " 'pet',\n",
       " 'poodle',\n",
       " 'puppy',\n",
       " 'retriever',\n",
       " 'dogs',\n",
       " 'rabbit']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get 10 closest words to the target word \"cat\" from Seinfeld word list\n",
    "spacy_closest(seinfeld_tokens,vec(\"cat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bodega',\n",
       " 'deli',\n",
       " 'delicatessen',\n",
       " 'delicatessens',\n",
       " 'knish',\n",
       " 'appetizers',\n",
       " 'pastrami',\n",
       " 'pastries',\n",
       " 'sandwiches',\n",
       " 'artisan',\n",
       " 'bakeries',\n",
       " 'bakery',\n",
       " 'supermarket',\n",
       " 'grocery',\n",
       " 'pizza']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_closest(seinfeld_tokens,vec(\"deli\"),15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coming together:\n",
    "\n",
    "# Seinfeld-themed song title transformer\n",
    "\n",
    "Let's create a fun method that transform any song title or lyrics into Seinfeld-themed word combination using our cloesest neighbors method we just created\n",
    "\n",
    "Make sure you have following libraries imported&downloaded:\n",
    "\n",
    "```python\n",
    "    from numpy import dot\n",
    "    from numpy.linalg import norm\n",
    "    from random import choice\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that takes in a sentence and a list of tokens\n",
    "def seinfeldTransformer(song_title,word_tokens,num = 2):\n",
    "    \n",
    "    # Vectorize Function\n",
    "    def vec(word):\n",
    "        return nlp.vocab[word].vector\n",
    "    \n",
    "    # Cosine Similarity Function\n",
    "    def cosine(v1, v2):\n",
    "        if norm(v1) > 0 and norm(v2) > 0:\n",
    "            return dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "        else:\n",
    "            return 0.0\n",
    "    \n",
    "    # Closest Neighbor Function\n",
    "    def spacy_closest(token_list, target_word_vec,n=10 ): \n",
    "        return sorted(token_list,key=lambda x: cosine(target_word_vec, vec(x)),reverse=True )[:n]\n",
    "    \n",
    "    # Replace current sentence input with a random most-similar word\n",
    "    def getNewTitle(sen, li):\n",
    "        word_list = sen.split(\" \")\n",
    "        new_list = []\n",
    "        for word in word_list:\n",
    "            replace_word = choice(spacy_closest(li,vec(word),10))\n",
    "            new_list.append(replace_word)\n",
    "        return ' '.join(new_list)\n",
    "    \n",
    "    # Generate\n",
    "    print('\"%s\" can also be called:' % (song_title))\n",
    "    for i in range(num):\n",
    "        print(\"---\")\n",
    "        print(\"%d. %s\" % (i+1, getNewTitle(song_title,word_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Another one bites the dust\" can also be called:\n",
      "---\n",
      "1. an it antidote its pebble\n",
      "---\n",
      "2. another only treats one debris\n",
      "---\n",
      "3. least that mauled that grime\n",
      "---\n",
      "4. a even fleas entire sand\n",
      "---\n",
      "5. one it bite its debris\n"
     ]
    }
   ],
   "source": [
    "# Queen?\n",
    "seinfeldTransformer(\"Another one bites the dust\",seinfeld_tokens,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"La vie en rose\" can also be called:\n",
      "---\n",
      "1. un ouf el rose\n",
      "---\n",
      "2. le elle y gardenias\n",
      "---\n",
      "3. de monsieur de rose\n",
      "---\n",
      "4. tes bonjour jour flower\n",
      "---\n",
      "5. responsable euh de plummeted\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "seinfeldTransformer(\"La vie en rose\",seinfeld_tokens,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"We are the champions\" can also be called:\n",
      "---\n",
      "1. they are part victory\n",
      "---\n",
      "2. let these one decathlon\n",
      "---\n",
      "3. what these part finals\n",
      "---\n",
      "4. they are part triumph\n",
      "---\n",
      "5. we those into decathlon\n"
     ]
    }
   ],
   "source": [
    "# more Queen?\n",
    "seinfeldTransformer(\"We are the champions\",seinfeld_tokens,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A fox is chasing a chicken\" can also be called:\n",
      "---\n",
      "1. second rabbit is pouncing another marsala\n",
      "---\n",
      "2. one wolf one tailing is brisket\n",
      "---\n",
      "3. another skunk is chasing the pork\n",
      "---\n",
      "4. kind skunk be pouncing something chowder\n",
      "---\n",
      "5. another rabbit comes chasing first meat\n"
     ]
    }
   ],
   "source": [
    "# or any sentence really:\n",
    "seinfeldTransformer(\"A fox is chasing a chicken\",seinfeld_tokens,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Red Hot Chilli Pepper\" can also be called:\n",
      "---\n",
      "1. orange hot niblets oregano\n",
      "---\n",
      "2. lilac wet niblets rosemary\n",
      "---\n",
      "3. aqua chick peppers garlic\n",
      "---\n",
      "4. lilac steamy pepper onion\n",
      "---\n",
      "5. pink steamy chiles oregano\n"
     ]
    }
   ],
   "source": [
    "# Need a new branding for your band?\n",
    "seinfeldTransformer(\"Red Hot Chilli Pepper\",seinfeld_tokens,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr>\n",
    "\n",
    "# Going Further\n",
    "\n",
    "- Use a different word list we provided in the folder: `../w2v_data_prep/jane_austen_emma.txt` \n",
    "    - There are notebooks in the same folder that demonstrate how the words are prepared.\n",
    "- Explore other Natural Language Processing Dataset through this [git repo](https://github.com/niderhoff/nlp-datasets)\n",
    "- Read Allison Parrish [\"Understanding Word Vectors\"](https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469)\n",
    "- If you want to read more about word vector, check out [Illustrated word2vec](http://jalammar.github.io/illustrated-word2vec/)\n",
    "- If you are interested in visualizing word vectors in a 2D vector space, check out [this notebook](https://github.com/parsons-python-summer-2020/python/tree/master/Week_08/word2vec/w2v_2d_plotting) by Anna & Lan\n",
    "\n",
    "Preview of a reduced 2D word vector space:\n",
    "<img src=\"./t-SNE.png\" alt=\"drawing\" width=\"800\" style=\"float: left;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
