{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Web Scraping\n",
    "\n",
    "\n",
    "Scraping is not always legal!  \n",
    "\n",
    "Some rules to consider: \n",
    "* Be respectful and do not bombard a website with scraping request or else you can get your IP address blocked\n",
    "* Check the website permission before you begin! If there is an API available, use it. Most websites won't let you use their data commercially.\n",
    "* Each website is unique and may update, so you may need to update your code and/or customize your scraping code for each website\n",
    "\n",
    "\n",
    "When is it a good idea to scrape a website:\n",
    "* API is not available, or information you want is not in the API\n",
    "* You want to anonoymously scrape a website (use a VPN) \n",
    "\n",
    "Here is a Web Scraping Sandbox where you can practice scraping: \n",
    "http://toscrape.com/\n",
    "\n",
    "Today, we're going to start with scraping www.wikipedia.com because it is *legal* to scrape\n",
    "\n",
    "This lesson was adapted from: https://github.com/Pierian-Data/Complete-Python-3-Bootcamp/blob/master/13-Web-Scraping/00-Guide-to-Web-Scraping.ipynb\n",
    "\n",
    "\n",
    "Make sure you download requests and bs4 via terminal \n",
    "\n",
    "* pip install requests\n",
    "* pip install bs4\n",
    "\n",
    "or if you're using Anaconda \n",
    "\n",
    "* conda install requests\n",
    "* conda install bs4\n",
    "\n",
    "or install it via notebook \n",
    "\n",
    "* !pip install requests\n",
    "* !pip install bs4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The request library will grab the page\n",
    "import requests\n",
    "\n",
    "response = requests.get(\"INSERT_WIKI_LINK_HERE\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The beautifulsoup library makes your code legible and helps you analyze the extracted page\n",
    "\n",
    "import bs4 \n",
    "soup = bs4.BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next inspect the elements on the wiki page, I want to grab the headlines \n",
    "# the headlines are in the class=\"mw-headline\" in a <span> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a list for the scrapped headlines \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save to a CSV, we first want to create a dataframe for the data\n",
    "\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're starting by going through the HTML and looking for something all the images have in common \n",
    "# the class 'thumbimage' applies to all the images \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're creating a list of the links for the thumbnails \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, I want to download these images on to my computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the images, we first want to create a dataframe for the data, we already imported Panda earlier \n",
    "\n",
    "# Creating a new dataframe \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST: Create a folder title Scraped_Images\n",
    "\n",
    "# We can use urllib to download the image urls\n",
    "import urllib.request\n",
    "# We want to add a sleeper to not get blocked \n",
    "import time \n",
    "\n",
    "# Iterate over DataFrame and download images "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
